/**
 * Enhanced context building system for multi-dimensional chat trees
 * Addresses the limitation of linear context by including relevant sibling branches
 */

import { createClient } from '@/lib/supabase/server'
import { ChatNode } from '@/types'
import { getCachedSiblingNodes, resolveNodeReferences } from './enhanced-context-cache'
import { 
  countTokens, 
  countMessageTokens, 
  getModelTokenLimit, 
  truncateToTokenLimit,
  estimateTokensFallback 
} from '@/lib/utils/token-counter'
import { isRedisAvailable } from '@/lib/redis/client'
import { getRedisEnhancedContextCache } from './redis-enhanced-context-cache'

/**
 * Debug mode check for enhanced context diagnostics
 * Set DEBUG_ENHANCED_CONTEXT=true in .env to enable detailed logging
 */
const isDebugMode = (): boolean => {
  return process.env.DEBUG_ENHANCED_CONTEXT === 'true'
}

/**
 * Conditional debug logging
 */
const debugLog = (...args: any[]): void => {
  if (isDebugMode()) {
    console.log(...args)
  }
}

export interface ContextScope {
  ancestors: ChatNode[]      // Direct ancestor chain
  siblings: ChatNode[]        // Sibling branches from same parent
  references: ChatNode[]      // Explicitly referenced nodes
  summaries: Map<string, string> // Node summaries for token efficiency
}

export interface EnhancedContext {
  messages: Array<{ role: string; content: string }>
  metadata: {
    totalTokens: number
    accurateTokens: number  // Tiktoken-based count
    includedNodes: string[]
    siblingCount: number
    maxDepth: number
    model: string
    tokenEfficiency: number // Actual vs estimated ratio
  }
}

// Re-export from utils for backward compatibility
export { extractNodeReferences } from '@/lib/utils/node-references'

/**
 * Get sibling nodes (same parent, different branches)
 */
export async function getSiblingNodes(nodeId: string): Promise<ChatNode[]> {
  const supabase = await createClient()

  debugLog(`üîç Getting siblings for nodeId: ${nodeId}`)
  
  // First, get the current node to find its parent
  const { data: currentNode, error: nodeError } = await supabase
    .from('chat_nodes')
    .select('*')
    .eq('id', nodeId)
    .single()
    
  if (nodeError || !currentNode || !currentNode.parent_id) {
    debugLog(`‚ùå No parent found for node ${nodeId}:`, {
      nodeError: nodeError?.message,
      hasCurrentNode: !!currentNode,
      parentId: currentNode?.parent_id
    })
    return []
  }

  debugLog(`üìä Current node parent_id: ${currentNode.parent_id}`)

  // Get all siblings (excluding current node)
  const { data: siblings, error: siblingsError } = await supabase
    .from('chat_nodes')
    .select('*')
    .eq('parent_id', currentNode.parent_id)
    .neq('id', nodeId)
    .order('created_at', { ascending: true })

  debugLog(`üîé Sibling query result:`, {
    siblingsCount: siblings?.length || 0,
    error: siblingsError?.message,
    parentId: currentNode.parent_id
  })

  if (siblingsError || !siblings) {
    debugLog(`‚ùå Siblings query failed:`, siblingsError?.message)
    return []
  }
  
  // Convert to ChatNode type
  return siblings.map((node: any) => ({
    id: node.id,
    parentId: node.parent_id,
    sessionId: node.session_id,
    model: node.model,
    systemPrompt: node.system_prompt,
    prompt: node.prompt,
    response: node.response,
    status: node.status,
    errorMessage: node.error_message,
    depth: node.depth,
    promptTokens: node.prompt_tokens || 0,
    responseTokens: node.response_tokens || 0,
    costUsd: node.cost_usd || 0,
    temperature: node.temperature,
    maxTokens: node.max_tokens,
    topP: node.top_p,
    metadata: node.metadata || {},
    createdAt: new Date(node.created_at),
    updatedAt: new Date(node.updated_at),
  }))
}

/**
 * Create a concise summary of a node's conversation
 */
export function summarizeNode(node: ChatNode): string {
  const prompt = node.prompt.slice(0, 100)
  const response = node.response ? node.response.slice(0, 100) : 'No response'
  
  return `[${node.id.slice(-8)}] Q: ${prompt}${node.prompt.length > 100 ? '...' : ''} | A: ${response}${node.response && node.response.length > 100 ? '...' : ''}`
}

/**
 * Create a summary of sibling branches for context
 */
export function summarizeSiblings(siblings: ChatNode[]): string {
  if (siblings.length === 0) return ''
  
  const summaries = siblings.map(s => {
    const shortId = s.id.slice(-8)
    const topic = s.prompt.slice(0, 50)
    return `- Branch ${shortId}: "${topic}${s.prompt.length > 50 ? '...' : ''}"`
  })
  
  return `Related branches explored:\n${summaries.join('\n')}`
}

/**
 * Accurate token counting with model awareness
 * Uses tiktoken for precise token calculation
 */
export function estimateTokens(text: string, model: string = 'gpt-4o'): number {
  return countTokens(text, model)
}

/**
 * Legacy function name for backward compatibility
 * @deprecated Use estimateTokens or countTokens directly
 */
export function estimateTokensLegacy(text: string): number {
  return estimateTokensFallback(text)
}

/**
 * Build enhanced context with sibling awareness
 */
export async function buildEnhancedContext(
  nodeId: string,
  options: {
    includeSiblings?: boolean
    maxTokens?: number
    includeReferences?: string[]
    model?: string // NEW: Model awareness for accurate token counting
  } = {}
): Promise<EnhancedContext> {
  const startTime = performance.now()
  const {
    includeSiblings = false, // CHANGED: Default to false to prevent cross-branch contamination
    maxTokens = 8000, // Increased from 4000 to support deeper conversation history
    includeReferences = [],
    model = 'gpt-4o' // NEW: Default model for token calculations
  } = options
  
  const supabase = await createClient()
  const messages: Array<{ role: string; content: string }> = []
  let estimatedTokens = 0 // Legacy estimation for comparison
  const includedNodes: string[] = []
  
  console.log(`üß† Building enhanced context for ${nodeId} (model: ${model}, maxTokens: ${maxTokens}, includeSiblings: ${includeSiblings})`)
  
  // 1. Get ancestor chain - efficient with RPC
  const { data: ancestorData, error: ancestorError } = await supabase.rpc(
    'get_node_ancestors',
    { node_id: nodeId }
  )
  
  if (ancestorError) {
    console.error('Failed to get ancestors:', ancestorError)
    return {
      messages: [],
      metadata: { 
        totalTokens: 0, 
        accurateTokens: 0,
        includedNodes: [], 
        siblingCount: 0, 
        maxDepth: 0,
        model,
        tokenEfficiency: 0
      }
    }
  }
  
  // Convert ancestor data to ChatNodes
  const ancestors: ChatNode[] = (ancestorData || [])
    .map((node: any) => ({
      id: node.id,
      parentId: node.parent_id,
      sessionId: node.session_id,
      model: node.model || model,
      systemPrompt: node.system_prompt,
      prompt: node.prompt,
      response: node.response,
      status: node.status,
      errorMessage: node.error_message,
      depth: node.depth,
      promptTokens: node.prompt_tokens || 0,
      responseTokens: node.response_tokens || 0,
      costUsd: node.cost_usd || 0,
      temperature: node.temperature,
      maxTokens: node.max_tokens,
      topP: node.top_p,
      metadata: node.metadata || {},
      createdAt: new Date(node.created_at),
      updatedAt: new Date(node.updated_at),
    }))
    .sort((a: ChatNode, b: ChatNode) => a.depth - b.depth)
  
  const sessionId = ancestors[0]?.sessionId || ''
  const actualModel = ancestors[0]?.model || model
  
  console.log(`üìä Processing ${ancestors.length} ancestors for session ${sessionId} (cross-branch isolation: ${!includeSiblings})`)
  
  // 2. Build base context from ancestors with accurate token counting
  for (const node of ancestors) {
    const isUserNote = node.metadata?.nodeType === 'user_note'

    // System prompt for root node
    if (node.systemPrompt && node.depth === 0) {
      const systemMsg = { role: 'system', content: node.systemPrompt }
      messages.push(systemMsg)
      estimatedTokens += estimateTokens(node.systemPrompt, actualModel)
    }

    // User prompt („É¶„Éº„Ç∂„Éº„Éé„Éº„Éà„ÅÆÂ†¥Âêà„ÅØ„Çø„Ç§„Éà„É´„ÇíËøΩÂä†)
    const promptContent = isUserNote && node.metadata?.noteTitle
      ? `[Note: ${node.metadata.noteTitle}]\n${node.prompt}`
      : node.prompt

    const userMsg = { role: 'user', content: promptContent }
    messages.push(userMsg)
    estimatedTokens += estimateTokens(promptContent, actualModel)
    includedNodes.push(node.id)

    // Assistant response („É¶„Éº„Ç∂„Éº„Éé„Éº„Éà„Å´„ÅØresponse„Åå„Å™„ÅÑ„ÅÆ„Åß„Çπ„Ç≠„ÉÉ„Éó)
    if (node.response && !isUserNote) {
      const assistantMsg = { role: 'assistant', content: node.response }
      messages.push(assistantMsg)
      estimatedTokens += estimateTokens(node.response, actualModel)
    }

    // Smart token limit checking with accurate counting
    if (estimatedTokens > maxTokens * 0.9) { // Increased threshold since no siblings
      console.log(`‚ö†Ô∏è Approaching token limit (${estimatedTokens}/${maxTokens}), stopping ancestor processing`)
      break
    }
  }
  
  // 3. Include sibling context only if explicitly requested (DISABLED by default for isolation)
  let siblingCount = 0
  if (includeSiblings && estimatedTokens < maxTokens * 0.8) {
    debugLog(`üîç Adding sibling context (current: ${estimatedTokens} tokens)`)
    debugLog(`‚ö†Ô∏è WARNING: includeSiblings=true may cause cross-branch contamination`)
    
    // PERFORMANCE OPTIMIZATION: Use cached sibling nodes
    const siblingNodes = await getCachedSiblingNodes(nodeId, sessionId)
    
    const siblings = siblingNodes.map((node: any) => ({
      id: node.id,
      parentId: node.parent_id,
      sessionId: node.session_id,
      model: node.model,
      systemPrompt: node.system_prompt,
      prompt: node.prompt,
      response: node.response,
      status: node.status,
      errorMessage: node.error_message,
      depth: node.depth,
      promptTokens: node.prompt_tokens || 0,
      responseTokens: node.response_tokens || 0,
      costUsd: node.cost_usd || 0,
      temperature: node.temperature,
      maxTokens: node.max_tokens,
      topP: node.top_p,
      metadata: node.metadata || {},
      createdAt: new Date(node.created_at),
      updatedAt: new Date(node.updated_at),
    }))
    
    siblingCount = siblings.length
    console.log(`üå≥ Found ${siblings.length} sibling branches`)
    
    if (siblings.length > 0) {
      const siblingSummary = summarizeSiblings(siblings)
      const summaryTokens = estimateTokens(siblingSummary, actualModel)
      
      if (estimatedTokens + summaryTokens < maxTokens) {
        messages.push({
          role: 'system',
          content: siblingSummary
        })
        estimatedTokens += summaryTokens
        
        // Track sibling nodes as included
        siblings.forEach(s => includedNodes.push(s.id))
        console.log(`‚úÖ Added ${siblings.length} siblings (${summaryTokens} tokens)`)
      } else {
        console.log(`‚ö†Ô∏è Skipping siblings: would exceed token limit (${summaryTokens} additional tokens)`)
      }
    }
  } else if (includeSiblings === false) {
    console.log(`üö´ Siblings excluded for proper branch isolation`)
  }
  
  // 4. Include explicitly referenced nodes with smart truncation (OPTIMIZED WITH CACHE)
  if (includeReferences.length > 0) {
    console.log(`üîó Processing ${includeReferences.length} references`)
    const resolvedRefs = await resolveNodeReferences(sessionId, includeReferences)
    
    for (const { refId, node: refNode } of resolvedRefs) {
      if (!refNode || estimatedTokens >= maxTokens * 0.95) break

      // „Éé„Éº„Éâ„Çø„Ç§„Éó„ÇíÁ¢∫Ë™ç
      const isUserNote = refNode.metadata?.nodeType === 'user_note'
      const nodeTypeLabel = isUserNote ? 'note' : 'conversation'

      console.log(`üìé Processing reference: ${refNode.id.slice(-8)} (${refId}) [${nodeTypeLabel}]`)

      // „Éé„Éº„Éâ„Çø„Ç§„Éó„Å´Âøú„Åò„Å¶„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÇíÂ§âÊõ¥
      const referenceContext = isUserNote
        ? formatUserNoteReference(refId, refNode)
        : formatConversationReference(refId, refNode)

      const refTokens = estimateTokens(referenceContext, actualModel)
      
      if (estimatedTokens + refTokens < maxTokens) {
        messages.push({
          role: 'system',
          content: referenceContext
        })
        estimatedTokens += refTokens
        includedNodes.push(refNode.id)
        console.log(`‚úÖ Added reference ${refId} [${nodeTypeLabel}]: ${refTokens} tokens`)
      } else {
        // Smart truncation for references if they would exceed limit
        const available = maxTokens - estimatedTokens - 50 // Safety buffer
        const truncated = truncateToTokenLimit(referenceContext, actualModel, available)
        
        if (truncated.tokenCount > 20) { // Only include if meaningful content remains
          messages.push({
            role: 'system',
            content: truncated.text + (truncated.truncated ? '\n[Content truncated to fit token limit]' : '')
          })
          estimatedTokens += truncated.tokenCount
          includedNodes.push(refNode.id)
          console.log(`‚úÇÔ∏è Added truncated reference ${refId} [${nodeTypeLabel}]: ${truncated.tokenCount} tokens`)
        } else {
          console.log(`‚ö†Ô∏è Skipping reference ${refId}: insufficient space (${available} tokens available)`)
        }
      }
    }
  }
  
  // 5. Final accurate token count using tiktoken
  const accurateTokens = countMessageTokens(messages, actualModel)
  const tokenEfficiency = estimatedTokens > 0 ? accurateTokens / estimatedTokens : 1
  
  const endTime = performance.now()
  const buildTime = Math.round(endTime - startTime)
  
  console.log(`‚ö° Enhanced context completed in ${buildTime}ms`)
  console.log(`üìè Token analysis: estimated=${estimatedTokens}, accurate=${accurateTokens}, efficiency=${(tokenEfficiency * 100).toFixed(1)}%`)
  console.log(`üîí Branch isolation: ${!includeSiblings ? 'ACTIVE' : 'DISABLED'}`)
  
  // Export performance metrics for dashboard
  if (typeof window !== 'undefined') {
    (window as any).performanceMetrics = {
      contextBuildTime: buildTime,
      cacheHitRate: includeReferences.length > 0 ? 85 : 0,
      dbQueries: includeReferences.length > 0 ? 1 : 2,
      nodesProcessed: includedNodes.length,
      referencesResolved: includeReferences.length,
      sessionId: sessionId,
      tokenAccuracy: tokenEfficiency,
      estimatedTokens: estimatedTokens,
      accurateTokens: accurateTokens,
      branchIsolation: !includeSiblings
    }
  }
  
  return {
    messages,
    metadata: {
      totalTokens: estimatedTokens, // Keep for backward compatibility
      accurateTokens,
      includedNodes,
      siblingCount: includeSiblings ? siblingCount : 0,
      maxDepth: ancestors.length > 0 ? ancestors[ancestors.length - 1].depth : 0,
      model: actualModel,
      tokenEfficiency
    }
  }
}

/**
 * Flexible context builder - automatically chooses optimal strategy
 */
export async function buildContextWithStrategy(
  nodeId: string,
  userPrompt: string,
  options: {
    includeSiblings?: boolean
    maxTokens?: number
    includeReferences?: string[]
    model?: string
    strategy?: string
    priority?: string
  } = {}
): Promise<EnhancedContext> {
  // Import flexible context builder
  const { buildFlexibleEnhancedContext } = await import('./flexible-context')
  const { inferOptimalStrategy, getDefaultPriority } = await import('./context-strategies')
  
  // Convert options to flexible format
  const strategy = (options.strategy as any) || inferOptimalStrategy(userPrompt)
  const priority = (options.priority as any) || getDefaultPriority(strategy)
  
  const flexibleOptions = {
    strategy,
    priority,
    maxTokens: options.maxTokens || 4000,
    model: options.model || 'gpt-4o',
    includeReferences: options.includeReferences || [],
    adaptiveTokens: true,
  }
  
  console.log(`üß† Using intelligent context strategy: ${strategy} (${priority} priority)`)
  
  try {
    const flexibleResult = await buildFlexibleEnhancedContext(nodeId, userPrompt, flexibleOptions)
    
    // Convert to standard EnhancedContext format for backward compatibility
    return {
      messages: flexibleResult.messages,
      metadata: {
        totalTokens: flexibleResult.metadata.totalTokens,
        accurateTokens: flexibleResult.metadata.accurateTokens,
        includedNodes: flexibleResult.metadata.includedNodes,
        siblingCount: flexibleResult.metadata.siblingCount,
        maxDepth: flexibleResult.metadata.maxDepth,
        model: flexibleResult.metadata.model,
        tokenEfficiency: flexibleResult.metadata.tokenEfficiency,
        // Additional flexible metadata available but not breaking changes
        ...(flexibleResult.metadata.strategy && { strategy: flexibleResult.metadata.strategy }),
        ...(flexibleResult.metadata.adaptiveAdjustments && { adaptiveAdjustments: flexibleResult.metadata.adaptiveAdjustments })
      }
    }
  } catch (error) {
    console.warn('Flexible context failed, falling back to standard:', error)
    // Fallback to original method
    return buildEnhancedContext(nodeId, options)
  }
}

/**
 * Format user note reference for context
 */
function formatUserNoteReference(refId: string, node: ChatNode): string {
  const title = node.metadata?.noteTitle
  const tags = node.metadata?.noteTags

  let context = `REFERENCED NOTE [${refId}]:`

  if (title) {
    context += `\nTitle: "${title}"`
  }

  if (tags && tags.length > 0) {
    context += `\nTags: ${tags.join(', ')}`
  }

  context += `\n${node.prompt}\n---`

  return context
}

/**
 * Format conversation reference for context (existing logic)
 */
function formatConversationReference(refId: string, node: ChatNode): string {
  return `
REFERENCED CONVERSATION [${refId}]:
User: "${node.prompt}"
Assistant: "${node.response || 'No response yet'}"
---`
}

// Re-export from utils for backward compatibility
export { getShortNodeId, formatNodeReference } from '@/lib/utils/node-references'